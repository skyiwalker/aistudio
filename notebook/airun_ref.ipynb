{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##*#-*- encoding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sdr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##*\n",
    "def _remove_abnormal(all_data):\n",
    "    ## Pre-processing - 1. Input Missing values\n",
    "    ## There are many options in order to deal with a missing value such as:\n",
    "    ## - A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "    ## - A value from another randomly selected record.\n",
    "    ## - A mean, median or mode value for the column.\n",
    "    ## - A value estimated by another predictive model.\n",
    "    ## We are going to replace a missing value to 0 for numeric columns and 'empty' for string columns, respectively.\n",
    "\n",
    "    ## Filling for missing values\n",
    "    categorical_columns_list = []\n",
    "    for each in all_data.columns:\n",
    "        if all_data[each].dtypes != 'int64':\n",
    "            if all_data[each].dtypes != 'float64':\n",
    "                categorical_columns_list.append(each)\n",
    "    for item in categorical_columns_list:\n",
    "        all_data[item].fillna(\"empty\",inplace=True)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##*\n",
    "def _pre_feature(all_data, processingFeature):\n",
    "    ## Pre-processing - 2. Vectorization for Input\n",
    "    column_list = ['Cl', 'thickness', 'Umach', 'AOA', 'RE']\n",
    "    features = ['thickness', 'Umach', 'AOA', 'RE']\n",
    "    target = 'Cl'\n",
    "\n",
    "    ## Listing features and scaling datasets\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler_fit = scaler.fit(all_data[features])\n",
    "    results = scaler_fit.transform(processingFeature[features])\n",
    "    return pd.DataFrame(results, columns=features)\n",
    "##*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_target(all_data, processingTarget):\n",
    "    ## Pre-processing - 2. Vectorization for Output\n",
    "    target = 'Cl'\n",
    "    ## Output Column has Numeric data. There is nothing to be vectorized.\n",
    "\n",
    "    return processingTarget\n",
    "##*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _post_target(all_data, predicted): \n",
    "    ## Post-processing for Output\n",
    "    target = 'Cl'\n",
    "    ## For the Regression task, We do nothing here\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##* Data Loading\n",
    "try:\n",
    "    all_data = pd.read_csv(\"./feature.csv\")\n",
    "except:\n",
    "    all_data = pd.read_csv(\"./feature.csv\", encoding = \"ISO-8859-1\")\n",
    "## Discover the head of your data\n",
    "all_data.head()\n",
    "target = 'Cl'\n",
    "##* Preprocess the feature and target values in data\n",
    "all_data = _remove_abnormal(all_data)\n",
    "xy = all_data.copy()\n",
    "y = xy.loc[:, [target]]\n",
    "x = xy.drop(target, axis=1)\n",
    "processedX = _pre_feature(all_data, x)\n",
    "features = processedX.columns.tolist()\n",
    "processedY = _pre_target(all_data, y)\n",
    "scaled_all_data = pd.concat([processedX, processedY], axis=1)\n",
    "##* Train and Test Dataset Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(scaled_all_data, test_size = 0.2, shuffle = True)\n",
    "##* Random Forests Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(criterion = \"mse\", max_depth = None, min_samples_split = 2, n_estimators = 100, min_samples_leaf = 1)\n",
    "# Quality of a split, The max. depth of the tree, The min. num required to split an internal node, Number of trees, The max. num required to be at a leaf node\n",
    "clf.fit(train_data[features], train_data[target])\n",
    "\n",
    "#- [OPTION_NAME = (default value)]\n",
    "#- n_estimators = 100 // int, optional. The number of trees in the forest.\n",
    "#- criterion // string, optional. The function to measure the quality of a split.\n",
    "#- criterion = 'mse' // string, optional. For regression only. 'mse' and 'mae' are supported.\n",
    "#- criterion = 'gini' // string, optional. For classification only. 'gini' and 'entropy' are supported.\n",
    "#- max_depth = None // int or None, optional. The maximum depth of the tree. If none, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "#- min_samples_split = 2 // int, optional. The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "#- min_samples_leaf = 1 // int, optional. The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "#- You can find parameters details at\n",
    "#- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "#- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "## results\n",
    "print(clf)\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "##* Predict using the model we made.\n",
    "predicted = clf.predict(test_data[features])\n",
    "confidence = clf.score(test_data[features], test_data[target])\t# Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"Prediction accuracy: \", confidence)\n",
    "\n",
    "## Comparing output data of the testing datasets(never learned) with the predicted output data using input data of the testing datasets.\n",
    "vs = pd.DataFrame(np.c_[predicted, test_data[target]], columns=['Predicted', 'Actual'])\n",
    "print(vs.head())\n",
    "##* Plotting testing datasets vs. predicted datasets\n",
    "vsplot, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "ax.scatter(x = predicted, y = test_data[target], color='c', edgecolors=(0, 0, 0))\n",
    "ax.plot([test_data[target].min(), test_data[target].max()], [test_data[target].min(), test_data[target].max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()\n",
    "######################################################################\n",
    "## You have to save the AI Model for submitting your model\n",
    "## You have to prepare a parameter: model\n",
    "## (trained model that you want to register)\n",
    "## After you call this method, a 'model.h5' or 'model.pkl' file will be created. Do not change the file name!\n",
    "######################################################################\n",
    "aiModel = sdr.AIModel()\n",
    "aiModel.saveAIModel(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
