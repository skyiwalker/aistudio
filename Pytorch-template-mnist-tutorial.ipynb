{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KISTI\\Anaconda3\\envs\\ai\\python.exe\n"
     ]
    }
   ],
   "source": [
    "condapath = sys.executable\n",
    "print(condapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'ANSYS191_DIR': 'C:\\\\Program Files\\\\ANSYS Inc\\\\v191\\\\ANSYS',\n",
       "        'ANSYSLIC_DIR': 'C:\\\\Program Files\\\\ANSYS Inc\\\\Shared Files\\\\Licensing',\n",
       "        'ANSYS_SYSDIR': 'winx64',\n",
       "        'ANSYS_SYSDIR32': 'win32',\n",
       "        'ANS_OLD_ATTACH': '1',\n",
       "        'APPDATA': 'C:\\\\Users\\\\KISTI\\\\AppData\\\\Roaming',\n",
       "        'APR_ICONV_PATH': 'C:\\\\AutoSet9\\\\Server\\\\bin\\\\iconv\\\\',\n",
       "        'AWP_LOCALE191': 'en-us',\n",
       "        'AWP_ROOT191': 'C:\\\\Program Files\\\\ANSYS Inc\\\\v191',\n",
       "        'CADOE_LIBDIR191': 'C:\\\\Program Files\\\\ANSYS Inc\\\\v191\\\\CommonFiles\\\\Language\\\\en-us',\n",
       "        'CLASSPATH': 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_111\\\\lib;.',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'K0878P1',\n",
       "        'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe',\n",
       "        'CUDA_PATH': 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.2',\n",
       "        'CUDA_PATH_V10_2': 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.2',\n",
       "        'CUDA_PATH_V7_5': 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v7.5',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'FPS_BROWSER_APP_PROFILE_STRING': 'Internet Explorer',\n",
       "        'FPS_BROWSER_USER_PROFILE_STRING': 'Default',\n",
       "        'FP_NO_HOST_CHECK': 'NO',\n",
       "        'GTK_BASEPATH': 'C:\\\\Program Files (x86)\\\\GtkSharp\\\\2.12\\\\',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\KISTI',\n",
       "        'ICEMCFD_ROOT191': 'C:\\\\Program Files\\\\ANSYS Inc\\\\v191\\\\icemcfd',\n",
       "        'ICEMCFD_SYSDIR': 'win64_amd',\n",
       "        'JAVA_HOME': 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_111',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\KISTI\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\K0878P1',\n",
       "        'LSTC_LICENSE': 'ANSYS',\n",
       "        'MOZ_PLUGIN_PATH': 'C:\\\\PROGRAM FILES (X86)\\\\FOXIT SOFTWARE\\\\FOXIT READER\\\\plugins\\\\',\n",
       "        'MSMPI_BIN': 'C:\\\\Program Files\\\\Microsoft MPI\\\\Bin\\\\',\n",
       "        'NUMBER_OF_PROCESSORS': '8',\n",
       "        'NVCUDASAMPLES10_2_ROOT': 'C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v10.2',\n",
       "        'NVCUDASAMPLES7_5_ROOT': 'C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v7.5',\n",
       "        'NVCUDASAMPLES_ROOT': 'C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v10.2',\n",
       "        'NVTOOLSEXT_PATH': 'C:\\\\Program Files\\\\NVIDIA Corporation\\\\NvToolsExt\\\\',\n",
       "        'ONEDRIVE': 'C:\\\\Users\\\\KISTI\\\\OneDrive',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai;C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai\\\\Library\\\\bin;C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai\\\\Scripts;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.2\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.2\\\\libnvvp;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files\\\\Microsoft MPI\\\\Bin\\\\;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v7.5\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v7.5\\\\libnvvp;C:\\\\Python\\\\Python27\\\\;C:\\\\Python\\\\Python27\\\\Scripts;C:\\\\Program Files (x86)\\\\Intel\\\\iCLS Client\\\\;C:\\\\Program Files\\\\Intel\\\\iCLS Client\\\\;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files (x86)\\\\Intel\\\\Intel(R) Management Engine Components\\\\DAL;C:\\\\Program Files\\\\Intel\\\\Intel(R) Management Engine Components\\\\DAL;C:\\\\Program Files (x86)\\\\Intel\\\\Intel(R) Management Engine Components\\\\IPT;C:\\\\Program Files\\\\Intel\\\\Intel(R) Management Engine Components\\\\IPT;C:\\\\Program Files (x86)\\\\Skype\\\\Phone\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files (x86)\\\\Windows Kits\\\\8.1\\\\Windows Performance Toolkit\\\\;C:\\\\Program Files\\\\TortoiseSVN\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;D:\\\\Software\\\\dex2jar-2.0;C:\\\\Program Files\\\\Java\\\\jdk1.8.0_111\\\\bin;C:\\\\Program Files (x86)\\\\GtkSharp\\\\2.12\\\\bin;C:\\\\Program Files\\\\PuTTY\\\\;C:\\\\AutoSet9\\\\Server\\\\conf\\\\;C:\\\\AutoSet9\\\\Server\\\\bin\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\TortoiseGit\\\\bin;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files (x86)\\\\Visual Leak Detector\\\\bin\\\\Win32;C:\\\\Program Files (x86)\\\\Visual Leak Detector\\\\bin\\\\Win64;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2019.5.0\\\\;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\Program Files (x86)\\\\ZeinSoft\\\\ComVoy\\\\2.0\\\\agent;C:\\\\Python\\\\Python27\\\\Scripts;C:\\\\Python\\\\Python36-32\\\\Scripts\\\\;C:\\\\Python\\\\Python36-32\\\\;C:\\\\Users\\\\KISTI\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Program Files\\\\Bandizip\\\\;C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\Remote Debugger\\\\x64;D:\\\\_Development\\\\KISTI\\\\Hemos\\\\hemosk\\\\HemosStructure\\\\Executable;C:\\\\Users\\\\KISTI\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\KISTI\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': '5e03',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PSMODULEPATH': 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'P_SCHEMA': 'C:\\\\Program Files\\\\ANSYS Inc\\\\v191\\\\AISOL\\\\CADIntegration\\\\Parasolid\\\\PSchema',\n",
       "        'SESSIONNAME': 'Console',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\WINDOWS',\n",
       "        'TEMP': 'C:\\\\Users\\\\KISTI\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\KISTI\\\\AppData\\\\Local\\\\Temp',\n",
       "        'USERDOMAIN': 'K0878P1',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'K0878P1',\n",
       "        'USERNAME': 'KISTI',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\KISTI',\n",
       "        'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
       "        'VS120COMNTOOLS': 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 12.0\\\\Common7\\\\Tools\\\\',\n",
       "        'VS140COMNTOOLS': 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\Tools\\\\',\n",
       "        'WINDIR': 'C:\\\\WINDOWS',\n",
       "        'WINDOWS_TRACING_FLAGS': '3',\n",
       "        'WINDOWS_TRACING_LOGFILE': 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log',\n",
       "        'CONDA_PREFIX': 'C:\\\\Users\\\\KISTI\\\\Anaconda3\\\\envs\\\\ai',\n",
       "        'KERNEL_LAUNCH_TIMEOUT': '40',\n",
       "        'GIT_PYTHON_REFRESH': 'quiet',\n",
       "        'JPY_INTERRUPT_EVENT': '2588',\n",
       "        'IPY_INTERRUPT_EVENT': '2588',\n",
       "        'JPY_PARENT_PID': '2592',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./train.py\n",
    "\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start template code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DataSet\n",
    "dataset download(.gz file) by request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign MNIST numpy array dataset using `pickle` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 x 28, and is being stored as a flattened row of length\n",
    "784 (=28x28). Let's take a look at one; we need to reshape it to 2d\n",
    "first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good to see the first MNIST hand written data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n",
    "convert our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KISTI\\Anaconda3\\envs\\ai\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print to verify Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define loss function and output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Deep Nueral Netword(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Logistic model using Parameter\n",
    "'''\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "'''\n",
    "# Logistic model using Linear\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # batch size\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use optim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use DataLoader and TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip\n",
    "`zip(*iterable)`은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수이다.\n",
    "\n",
    "※ 여기서 사용한 `*iterable`은 반복 가능(iterable)한 자료형 여러 개를 입력할 수 있다는 의미이다.\n",
    "\n",
    "아래 예시 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6)]\n",
      "[(1, 4, 7), (2, 5, 8), (3, 6, 9)]\n",
      "(1, 4, 7) (2, 5, 8) (3, 6, 9)\n",
      "<class 'tuple'>\n",
      "[('a', 'd'), ('b', 'e'), ('c', 'f')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3], [4, 5, 6])))\n",
    "print(list(zip([1, 2, 3], [4, 5, 6], [7, 8, 9])))\n",
    "a,b,c = zip([1, 2, 3], [4, 5, 6], [7, 8, 9])\n",
    "print(a,b,c)\n",
    "print(type(a))\n",
    "print(list(zip(\"abc\", \"def\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting data and model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3579676120758057\n",
      "1 0.48495064752101896\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Python file to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./train.py\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Logistic model using Linear\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "        \n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--fp16-allreduce', action='store_true', default=False,\n",
    "                    help='use fp16 compression during allreduce')\n",
    "parser.add_argument('--use-adasum', action='store_true', default=False,\n",
    "                    help='use adasum algorithm to do reduction')\n",
    "args = parser.parse_args()\n",
    "\n",
    "############### Load Dataset ###############\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "############### Mapping PyTorch Tensors ###############\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "############### Define Loss Function ###############\n",
    "loss_func = F.cross_entropy        \n",
    "\n",
    "############### HyperParameters ###############\n",
    "# bs = 64  # batch size\n",
    "batch_size = args.batch_size # from arguments\n",
    "test_batch_size = args.test_batch_size\n",
    "# lr = 0.5  # learning rate\n",
    "lr = args.lr # from arguments\n",
    "# epochs = 2  # how many epochs to train for\n",
    "epochs = args.epochs # from arguments\n",
    "\n",
    "############### Setting Data ###############\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size)\n",
    "\n",
    "############### Getting Model ###############\n",
    "model, opt = get_model()\n",
    "\n",
    "############### Training Model ###############\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code (Fixed Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./train_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./train_test.py\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Logistic model using Linear\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "############### Load Dataset ###############\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "############### Mapping PyTorch Tensors ###############\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "############### Define Loss Function ###############\n",
    "loss_func = F.cross_entropy        \n",
    "\n",
    "############### HyperParameters ###############\n",
    "batch_size = 64\n",
    "test_batch_size = 128\n",
    "lr = 0.1  # learning rate\n",
    "epochs = 10  # how many epochs to train for\n",
    "\n",
    "\n",
    "############### Setting Data ###############\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size)\n",
    "\n",
    "############### Getting Model ###############\n",
    "model, opt = get_model()\n",
    "\n",
    "############### Training Model ###############\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Version Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.435265, Accuracy: 88.195999\n",
      "Epoch: 2, Loss: 0.323936, Accuracy: 90.991997\n",
      "Epoch: 3, Loss: 0.305298, Accuracy: 91.324005\n",
      "Epoch: 4, Loss: 0.294997, Accuracy: 91.725998\n",
      "Epoch: 5, Loss: 0.288776, Accuracy: 91.931999\n",
      "Epoch: 6, Loss: 0.283973, Accuracy: 92.005997\n",
      "Epoch: 7, Loss: 0.280601, Accuracy: 92.118004\n",
      "Epoch: 8, Loss: 0.276476, Accuracy: 92.276001\n",
      "Epoch: 9, Loss: 0.275013, Accuracy: 92.356003\n",
      "Epoch: 10, Loss: 0.272137, Accuracy: 92.362000\n",
      "Epoch: 11, Loss: 0.270319, Accuracy: 92.419998\n",
      "Epoch: 12, Loss: 0.269036, Accuracy: 92.445999\n",
      "Epoch: 13, Loss: 0.267302, Accuracy: 92.515999\n",
      "Epoch: 14, Loss: 0.265623, Accuracy: 92.538002\n",
      "Epoch: 15, Loss: 0.264390, Accuracy: 92.585999\n",
      "Epoch: 16, Loss: 0.263262, Accuracy: 92.695999\n",
      "Epoch: 17, Loss: 0.262047, Accuracy: 92.598000\n",
      "Epoch: 18, Loss: 0.261711, Accuracy: 92.646004\n",
      "Epoch: 19, Loss: 0.260219, Accuracy: 92.722000\n",
      "Epoch: 20, Loss: 0.259152, Accuracy: 92.643997\n",
      "Epoch: 21, Loss: 0.258789, Accuracy: 92.765999\n",
      "Epoch: 22, Loss: 0.257693, Accuracy: 92.742004\n",
      "Epoch: 23, Loss: 0.257459, Accuracy: 92.853996\n",
      "Epoch: 24, Loss: 0.256726, Accuracy: 92.888000\n",
      "Epoch: 25, Loss: 0.255537, Accuracy: 92.905998\n",
      "Epoch: 26, Loss: 0.255219, Accuracy: 92.922005\n",
      "Epoch: 27, Loss: 0.254122, Accuracy: 92.949997\n",
      "Epoch: 28, Loss: 0.254106, Accuracy: 92.882004\n",
      "Epoch: 29, Loss: 0.253473, Accuracy: 92.913994\n",
      "Epoch: 30, Loss: 0.252781, Accuracy: 92.888000\n",
      "Epoch: 31, Loss: 0.252194, Accuracy: 92.984001\n",
      "Epoch: 32, Loss: 0.251445, Accuracy: 93.003998\n",
      "Epoch: 33, Loss: 0.251397, Accuracy: 93.040001\n",
      "Epoch: 34, Loss: 0.250920, Accuracy: 92.972000\n",
      "Epoch: 35, Loss: 0.250595, Accuracy: 92.996002\n",
      "Epoch: 36, Loss: 0.250078, Accuracy: 93.029999\n",
      "Epoch: 37, Loss: 0.249669, Accuracy: 93.040001\n",
      "Epoch: 38, Loss: 0.249207, Accuracy: 93.003998\n",
      "Epoch: 39, Loss: 0.248960, Accuracy: 93.120003\n",
      "Epoch: 40, Loss: 0.248463, Accuracy: 93.036003\n",
      "Epoch: 41, Loss: 0.248161, Accuracy: 93.092003\n",
      "Epoch: 42, Loss: 0.247982, Accuracy: 93.029999\n",
      "Epoch: 43, Loss: 0.247633, Accuracy: 93.068001\n",
      "Epoch: 44, Loss: 0.247247, Accuracy: 93.068001\n",
      "Epoch: 45, Loss: 0.246758, Accuracy: 93.152000\n",
      "Epoch: 46, Loss: 0.246398, Accuracy: 93.092003\n",
      "Epoch: 47, Loss: 0.246041, Accuracy: 93.103996\n",
      "Epoch: 48, Loss: 0.246062, Accuracy: 93.112000\n",
      "Epoch: 49, Loss: 0.245594, Accuracy: 93.172005\n",
      "Epoch: 50, Loss: 0.245555, Accuracy: 93.183998\n",
      "Epoch: 51, Loss: 0.245391, Accuracy: 93.142006\n",
      "Epoch: 52, Loss: 0.244344, Accuracy: 93.206001\n",
      "Epoch: 53, Loss: 0.244817, Accuracy: 93.180000\n",
      "Epoch: 54, Loss: 0.244374, Accuracy: 93.167999\n",
      "Epoch: 55, Loss: 0.243687, Accuracy: 93.092003\n",
      "Epoch: 56, Loss: 0.243250, Accuracy: 93.202003\n",
      "Epoch: 57, Loss: 0.243328, Accuracy: 93.242004\n",
      "Epoch: 58, Loss: 0.242962, Accuracy: 93.242004\n",
      "Epoch: 59, Loss: 0.242813, Accuracy: 93.288002\n",
      "Epoch: 60, Loss: 0.242671, Accuracy: 93.203995\n",
      "Epoch: 61, Loss: 0.242238, Accuracy: 93.309998\n",
      "Epoch: 62, Loss: 0.242544, Accuracy: 93.192001\n",
      "Epoch: 63, Loss: 0.241975, Accuracy: 93.253998\n",
      "Epoch: 64, Loss: 0.242006, Accuracy: 93.246002\n",
      "Epoch: 65, Loss: 0.241695, Accuracy: 93.257996\n",
      "Epoch: 66, Loss: 0.241411, Accuracy: 93.290001\n",
      "Epoch: 67, Loss: 0.241520, Accuracy: 93.202003\n",
      "Epoch: 68, Loss: 0.241355, Accuracy: 93.320000\n",
      "Epoch: 69, Loss: 0.240929, Accuracy: 93.276001\n",
      "Epoch: 70, Loss: 0.241094, Accuracy: 93.239998\n",
      "Epoch: 71, Loss: 0.240469, Accuracy: 93.363998\n",
      "Epoch: 72, Loss: 0.240815, Accuracy: 93.272003\n",
      "Epoch: 73, Loss: 0.240227, Accuracy: 93.304001\n",
      "Epoch: 74, Loss: 0.239892, Accuracy: 93.322006\n",
      "Epoch: 75, Loss: 0.240019, Accuracy: 93.334000\n",
      "Epoch: 76, Loss: 0.239279, Accuracy: 93.337997\n",
      "Epoch: 77, Loss: 0.239205, Accuracy: 93.307999\n",
      "Epoch: 78, Loss: 0.239491, Accuracy: 93.393997\n",
      "Epoch: 79, Loss: 0.239241, Accuracy: 93.388000\n",
      "Epoch: 80, Loss: 0.238845, Accuracy: 93.360001\n",
      "Epoch: 81, Loss: 0.238648, Accuracy: 93.279999\n",
      "Epoch: 82, Loss: 0.238982, Accuracy: 93.377998\n",
      "Epoch: 83, Loss: 0.238769, Accuracy: 93.367996\n",
      "Epoch: 84, Loss: 0.238185, Accuracy: 93.374001\n",
      "Epoch: 85, Loss: 0.238268, Accuracy: 93.375999\n",
      "Epoch: 86, Loss: 0.238106, Accuracy: 93.337997\n",
      "Epoch: 87, Loss: 0.237705, Accuracy: 93.400002\n",
      "Epoch: 88, Loss: 0.238010, Accuracy: 93.349998\n",
      "Epoch: 89, Loss: 0.237749, Accuracy: 93.452003\n",
      "Epoch: 90, Loss: 0.237831, Accuracy: 93.419998\n",
      "Epoch: 91, Loss: 0.237911, Accuracy: 93.428001\n",
      "Epoch: 92, Loss: 0.237671, Accuracy: 93.374001\n",
      "Epoch: 93, Loss: 0.237266, Accuracy: 93.405998\n",
      "Epoch: 94, Loss: 0.237169, Accuracy: 93.436005\n",
      "Epoch: 95, Loss: 0.236612, Accuracy: 93.437996\n",
      "Epoch: 96, Loss: 0.236523, Accuracy: 93.400002\n",
      "Epoch: 97, Loss: 0.236422, Accuracy: 93.458000\n",
      "Epoch: 98, Loss: 0.236813, Accuracy: 93.405998\n",
      "Epoch: 99, Loss: 0.236406, Accuracy: 93.386002\n",
      "Epoch: 100, Loss: 0.236311, Accuracy: 93.437996\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ./train_test.py\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Logistic model using Linear\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        nums = []\n",
    "        accuracies = 0.\n",
    "        for xb, yb in train_dl:\n",
    "            model.train()\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            model.eval()\n",
    "            losses.append(loss.item())\n",
    "            nums.append(len(xb))\n",
    "            acc = accuracy(preds, yb)\n",
    "            accuracies += acc*len(xb)\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)        \n",
    "        val_acc = accuracies / np.sum(nums)           \n",
    "        print(\"Epoch: {:d}, Loss: {:f}, Accuracy: {:f}\".format(epoch+1, val_loss, val_acc*100))\n",
    "\n",
    "############### Load Dataset ###############\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "############### Mapping PyTorch Tensors ###############\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "############### Define Loss Function ###############\n",
    "loss_func = F.cross_entropy        \n",
    "\n",
    "############### HyperParameters ###############\n",
    "batch_size = 64\n",
    "test_batch_size = 128\n",
    "lr = 0.2  # learning rate\n",
    "epochs = 100  # how many epochs to train for\n",
    "\n",
    "\n",
    "############### Setting Data ###############\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch_size)\n",
    "\n",
    "############### Getting Model ###############\n",
    "model, opt = get_model()\n",
    "\n",
    "############### Training Model ###############\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
