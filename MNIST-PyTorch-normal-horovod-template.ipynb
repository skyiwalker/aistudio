{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Code for PyTorch\n",
    "\n",
    "1. Normal Template Code\n",
    "2. Distributed/Parallel Template Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1 -> 2 Transformation Methods\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Module to get arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch Modules for distributed/parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data.distributed\n",
    "import horovod.torch as hvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: This is arguments set #####\n",
    "def set_hyperparameters():\n",
    "    ##### CUDA SETTING #####\n",
    "    no_cuda = False\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "    #########################\n",
    "    batch_size = 64\n",
    "    test_batch_size = 128\n",
    "    epochs = 10    \n",
    "    momentum = 0.5\n",
    "    lr = 0.01 # learning rate\n",
    "    log_interval = 10\n",
    "    # For Parallel/Distributed\n",
    "    seed = 42\n",
    "    use_adasum = False\n",
    "    ##########################\n",
    "    use_horovod = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horovod Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: initialize library.\n",
    "##### HOROVOD #####\n",
    "hvd.init()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if cuda:\n",
    "    # Horovod: pin GPU to local rank.\n",
    "    ##### HOROVOD #####\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    ##### TODO:Need argument #####\n",
    "    #torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Horovod: limit # of CPU threads to be used per worker.\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fit Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    ##### HOROVOD ##### --- train_sampler, optimizer wrapper\n",
    "    # Horovod: set epoch to sampler for shuffling.\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            ##### HOROVOD ##### --- train_sampler\n",
    "            # Horovod: use train_sampler to determine the number of examples in\n",
    "            # this worker's partition.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_sampler),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, optimizer, train_loader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "            ##### HOROVOD ##### --- train_sampler\n",
    "            # Horovod: use train_sampler to determine the number of examples in\n",
    "            # this worker's partition.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_sampler),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_average(val, name):\n",
    "    tensor = torch.tensor(val)\n",
    "    ##### HOROVOD ##### -- allreduce (tensor average)\n",
    "    avg_tensor = hvd.allreduce(tensor, name=name)    \n",
    "    return avg_tensor.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function (Horovod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    test_accuracy = 0.\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: use test_sampler to determine the number of examples in\n",
    "    # this worker's partition.\n",
    "    test_loss /= len(test_sampler)\n",
    "    test_accuracy /= len(test_sampler)\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: average metric values across workers.\n",
    "    test_loss = metric_average(test_loss, 'avg_loss')\n",
    "    test_accuracy = metric_average(test_accuracy, 'avg_accuracy')\n",
    "\n",
    "    # Horovod: print output only on first rank.\n",
    "    ##### HOROVOD #####\n",
    "    if hvd.rank() == 0:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "            test_loss, 100. * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function(Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    test_accuracy = 0.\n",
    "    for data, target in test_loader:        \n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: use test_sampler to determine the number of examples in\n",
    "    # this worker's partition.\n",
    "    test_loss /= len(test_sampler)\n",
    "    test_accuracy /= len(test_sampler)\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: average metric values across workers.\n",
    "    test_loss = metric_average(test_loss, 'avg_loss')\n",
    "    test_accuracy = metric_average(test_accuracy, 'avg_accuracy')\n",
    "\n",
    "    # Horovod: print output only on first rank.\n",
    "    ##### HOROVOD #####\n",
    "    if hvd.rank() == 0:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "            test_loss, 100. * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    set_hyperparameters()\n",
    "    kwargs = {}\n",
    "    if use_horovod:\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "        # When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
    "        # issues with Infiniband implementations that are not fork-safe\n",
    "        if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "                mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "            kwargs['multiprocessing_context'] = 'forkserver'\n",
    "            \n",
    "    if use_horovod:\n",
    "        ##### HOROVOD #####\n",
    "        train_dataset = \\\n",
    "        datasets.MNIST('data-%d' % hvd.rank(), train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "        ##### HOROVOD #####\n",
    "        # Horovod: use DistributedSampler to partition the training data.\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "        ##### TODO:Need argument #####\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs)\n",
    "        ##### HOROVOD #####\n",
    "        test_dataset = \\\n",
    "            datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ]))\n",
    "        ##### HOROVOD #####\n",
    "        # Horovod: use DistributedSampler to partition the test data.\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "        ##### TODO:Need argument #####\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                                  sampler=test_sampler, **kwargs)\n",
    "    else:\n",
    "        download_root = 'data'\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "        test_dataset = datasets.MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "        valid_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "    model = Net()\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # By default, Adasum doesn't need scaling up learning rate.\n",
    "    lr_scaler = hvd.size() if not use_adasum else 1\n",
    "\n",
    "    if cuda:\n",
    "        # Move model to GPU.\n",
    "        model.cuda()\n",
    "        # If using GPU Adasum allreduce, scale learning rate by local_size.\n",
    "        ##### TODO:Need argument #####\n",
    "        if use_adasum and hvd.nccl_built():\n",
    "            lr_scaler = hvd.local_size()\n",
    "\n",
    "    # Horovod: scale learning rate by lr_scaler.\n",
    "    ##### TODO:Need argument #####\n",
    "    '''\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr * lr_scaler,\n",
    "                          momentum=args.momentum)'''\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr * lr_scaler,\n",
    "                          momentum=momentum)\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: broadcast parameters & optimizer state.\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: (optional) compression algorithm.\n",
    "    # compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n",
    "    compression = hvd.Compression.none\n",
    "\n",
    "    ##### HOROVOD #####\n",
    "    # Horovod: wrap optimizer with DistributedOptimizer.\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer,\n",
    "                                         named_parameters=model.named_parameters(),\n",
    "                                         compression=compression,\n",
    "                                         op=hvd.Adasum if use_adasum else hvd.Average)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Codes Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "PyTorch Tensor dataset is composed of x(input), y(output/label) to use dataset easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "download_root = 'test-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: test-data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/anaconda3/envs/ai/lib/python3.7/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "PyTorch DataLoader is used to loop data easily.\n",
    "A loop contains dataset with mini batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "test_dataset = datasets.MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "valid_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f0bbc06ef90>\n",
      "938\n",
      "60032\n",
      "60000\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f0bbc06e8d0>\n",
      "157\n",
      "10048\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)\n",
    "print(len(train_loader))\n",
    "print(batch_size * len(train_loader))\n",
    "print(len(train_loader.dataset))\n",
    "print(test_loader)\n",
    "print(len(test_loader))\n",
    "print(batch_size * len(test_loader))\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "1\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "2\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "936\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "937\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n",
      "0\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "1\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "2\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "155\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "156\n",
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for idx, (xb, yb) in enumerate(train_loader):\n",
    "    if idx < 3:\n",
    "        print(idx)\n",
    "        print(xb.shape)\n",
    "        print(yb.shape)\n",
    "    if idx > 935:\n",
    "        print(idx)\n",
    "        print(xb.shape)\n",
    "        print(yb.shape)\n",
    "for idx, (xb, yb) in enumerate(test_loader):\n",
    "    if idx < 3:\n",
    "        print(idx)\n",
    "        print(xb.shape)\n",
    "        print(yb.shape)\n",
    "    if idx > 154:\n",
    "        print(idx)\n",
    "        print(xb.shape)\n",
    "        print(yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Getting Model Function because optimizer need model parameters\n",
    "optimizer --- SGD, AdaGrad, Momentum, Adam, RMSProp, ...\n",
    "\n",
    "optimizer is for calculating gradient to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "def get_model():\n",
    "    model = Net()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check trainloader length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, optimizer, train_loader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    test_accuracy = 0.    \n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data)\n",
    "        local_loss = loss_func(output, target)\n",
    "        local_accuracy = accuracy(output,target)\n",
    "        test_loss += local_loss * len(data)\n",
    "        test_accuracy += local_accuracy * len(data)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'\n",
    "          .format(test_loss,100.* test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.296645\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.277264\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.206328\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.247442\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.190287\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.277692\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.125373\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.243024\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.313523\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.451105\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.283882\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.169014\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.304851\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.243588\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.130000\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.145878\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.246605\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.260114\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148887\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.198462\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.174594\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.133955\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.144170\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.383072\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.246551\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.172845\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.214792\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.285527\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.340074\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.265671\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.181260\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.280929\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.221358\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.377996\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.260237\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.220887\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.237416\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.240888\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.277909\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.183521\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.342653\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.204755\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.260900\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.276405\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.310011\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.282397\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.141223\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.192497\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.167851\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.302769\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.107820\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.180280\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.325626\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.278812\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.204132\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.132657\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.277523\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.190574\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.128045\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.244007\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.221776\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.133700\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.293609\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.330916\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.242372\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.231297\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.343658\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.245948\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.193432\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.507038\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.336265\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.086797\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.138759\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.331362\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.164743\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.373251\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.097081\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.137725\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.197088\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.263393\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.292540\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.191684\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.297253\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.347965\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.267290\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.165470\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.154826\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.418506\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.197040\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.223690\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.325550\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.295051\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.151640\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.156814\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.090436\n",
      "\n",
      "Test set: Average loss: 0.0713, Accuracy: 97.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "log_interval = 50\n",
    "\n",
    "loss_log = []\n",
    "fit(epochs, model, loss_func, optimizer, train_loader)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load for Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "        # When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
    "        # issues with Infiniband implementations that are not fork-safe\n",
    "if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "    mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "    kwargs['multiprocessing_context'] = 'forkserver'\n",
    "##### HOROVOD #####\n",
    "train_dataset = \\\n",
    "    datasets.MNIST('data-%d' % hvd.rank(), train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "##### HOROVOD #####\n",
    "# Horovod: use DistributedSampler to partition the training data.\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "##### TODO:Need argument #####\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs)\n",
    "##### HOROVOD #####\n",
    "test_dataset = \\\n",
    "    datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]))\n",
    "##### HOROVOD #####\n",
    "# Horovod: use DistributedSampler to partition the test data.\n",
    "test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "               test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "##### TODO:Need argument #####\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                          sampler=test_sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Test Codes Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziplist = [loss_batch(model, loss_func, xb, yb) for xb, yb in test_loader]\n",
    "len(ziplist)\n",
    "# print(*ziplist)\n",
    "# print(ziplist)\n",
    "losses, nums = zip(*ziplist)\n",
    "print(losses)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/anaconda3/envs/ai/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----losses-----\n",
      "(2.319767475128174, 2.309004068374634, 2.3203701972961426, 2.2952890396118164, 2.3022823333740234, 2.317312479019165, 2.2929506301879883, 2.3257343769073486, 2.316655158996582, 2.3386337757110596, 2.3223235607147217, 2.292008876800537, 2.3388988971710205, 2.2959821224212646, 2.3093717098236084, 2.2928993701934814, 2.3463830947875977, 2.3816003799438477, 2.3068344593048096, 2.3595120906829834, 2.3470187187194824, 2.319748878479004, 2.2877745628356934, 2.31534743309021, 2.3542819023132324, 2.3139638900756836, 2.3232228755950928, 2.348968744277954, 2.320908308029175, 2.3423118591308594, 2.310112953186035, 2.2821013927459717, 2.32356595993042, 2.3086283206939697, 2.2934176921844482, 2.33429217338562, 2.283425807952881, 2.321873426437378, 2.282600164413452, 2.360349178314209, 2.316889524459839, 2.3511197566986084, 2.269629955291748, 2.317993402481079, 2.3207175731658936, 2.319084405899048, 2.3116507530212402, 2.3352508544921875, 2.3578150272369385, 2.3142197132110596, 2.30165958404541, 2.2669761180877686, 2.323016405105591, 2.3051655292510986, 2.3115649223327637, 2.3143138885498047, 2.3324806690216064, 2.3003249168395996, 2.298233985900879, 2.3412368297576904, 2.3684098720550537, 2.2981185913085938, 2.308098077774048, 2.2901570796966553, 2.3012428283691406, 2.3118393421173096, 2.3113880157470703, 2.2915730476379395, 2.3791654109954834, 2.337367534637451, 2.3454580307006836, 2.273233652114868, 2.283109664916992, 2.301849603652954, 2.342012882232666, 2.3156843185424805, 2.339879274368286, 2.2843029499053955, 2.310814142227173, 2.308171033859253, 2.32619047164917, 2.3778929710388184, 2.2364795207977295, 2.321765184402466, 2.339446783065796, 2.3327314853668213, 2.307326555252075, 2.3373870849609375, 2.3235180377960205, 2.3308703899383545, 2.3149068355560303, 2.3082826137542725, 2.247443675994873, 2.35542631149292, 2.3241453170776367, 2.2844300270080566, 2.357836961746216, 2.3325934410095215, 2.321850061416626, 2.331160545349121, 2.332944869995117, 2.347205877304077, 2.3371856212615967, 2.340834140777588, 2.2828166484832764, 2.299192428588867, 2.2826216220855713, 2.354119062423706, 2.3756766319274902, 2.337052822113037, 2.3409547805786133, 2.314160108566284, 2.270385265350342, 2.3354272842407227, 2.3008999824523926, 2.3483848571777344, 2.3384926319122314, 2.323525905609131, 2.3103768825531006, 2.3374881744384766, 2.3063838481903076, 2.337493419647217, 2.345541477203369, 2.282269239425659, 2.345907688140869, 2.2837564945220947, 2.277534246444702, 2.354172706604004, 2.3324809074401855, 2.2937991619110107, 2.316967725753784, 2.3150055408477783, 2.3210818767547607, 2.3052282333374023, 2.257047176361084, 2.3473868370056152, 2.3258352279663086, 2.32138729095459, 2.3068079948425293, 2.3235182762145996, 2.298288583755493, 2.3187265396118164, 2.3039886951446533, 2.3017044067382812, 2.305040121078491, 2.3327815532684326, 2.31341814994812, 2.289504289627075, 2.2898528575897217, 2.32327938079834, 2.28265380859375, 2.3341240882873535, 2.366018295288086, 2.3141157627105713, 2.3267300128936768, 2.2967891693115234, 2.2987473011016846)\n",
      "-----num-----\n",
      "(64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 16)\n",
      "-----sum of num-----\n",
      "10000\n",
      "Test Loss: 2.317835\n",
      "Test Loss: 363.885746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "# Output Layer & Loss Function\n",
    "loss_func = F.nll_loss\n",
    "\n",
    "# for xb, yb in test_loader:\n",
    "#     print(\"Output Size:{:d}, Label Size:{:d}\".format(len(model(xb)),len(yb)))\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):    \n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "lb = [loss_batch(model, loss_func, xb, yb) for xb, yb in test_loader]\n",
    "losses, num = zip(*lb)\n",
    "# print(lb)\n",
    "print('-----losses-----')\n",
    "print(losses)\n",
    "print('-----num-----')\n",
    "print(num)\n",
    "test_loss = np.sum(np.multiply(losses, num)) / np.sum(num)\n",
    "test_loss2 = np.sum(losses)\n",
    "print('-----sum of num-----')\n",
    "print(np.sum(num))\n",
    "print(\"Test Loss: {:f}\".format(test_loss))\n",
    "print(\"Test Loss: {:f}\".format(test_loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "aaa = np.arange(0,10)\n",
    "bbb = torch.tensor(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ad10b23c1c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.argmax(bbb,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 0, 1, 0, 0, 3, 1, 0, 0, 0, 8, 6, 1, 1, 0, 3, 0, 0, 0, 2, 7, 0, 0, 3,\n",
      "        1, 0, 2, 3, 4, 9, 3, 0, 0, 0, 0, 1, 2, 1, 0, 0, 9, 1, 1, 0, 2, 0, 2, 0,\n",
      "        2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 9, 0, 3, 0],\n",
      "       grad_fn=<NotImplemented>)\n",
      "tensor([[-2.2407, -2.2118, -2.3539, -2.2382, -2.4272, -2.4221, -2.1456, -2.4964,\n",
      "         -2.2202, -2.3283],\n",
      "        [-1.9271, -2.0202, -2.1700, -2.0995, -2.5577, -2.3716, -2.7399, -2.8670,\n",
      "         -2.1804, -2.5277],\n",
      "        [-2.1736, -2.1110, -2.2403, -2.4793, -2.4560, -2.5213, -2.1798, -2.2833,\n",
      "         -2.3858, -2.2854],\n",
      "        [-2.0224, -2.3911, -2.2722, -2.2848, -2.3098, -2.6901, -2.1472, -2.1753,\n",
      "         -2.3258, -2.5770],\n",
      "        [-2.1855, -2.2223, -2.2133, -2.2657, -2.4246, -2.3712, -2.2962, -2.4516,\n",
      "         -2.2840, -2.3479],\n",
      "        [-2.0782, -2.0930, -2.2328, -2.0661, -2.3032, -2.3943, -2.7454, -2.4158,\n",
      "         -2.5766, -2.3342],\n",
      "        [-2.3819, -2.1083, -2.2407, -2.2821, -2.3821, -2.4858, -2.1300, -2.2365,\n",
      "         -2.4744, -2.3835],\n",
      "        [-1.9846, -2.0400, -2.4179, -2.4531, -2.4857, -2.2913, -2.4832, -2.5827,\n",
      "         -2.2437, -2.2266],\n",
      "        [-2.0898, -2.3170, -2.2480, -2.2130, -2.4288, -2.4558, -2.4275, -2.2648,\n",
      "         -2.2743, -2.3662],\n",
      "        [-1.7673, -2.2324, -1.9783, -2.4197, -3.2828, -2.4269, -2.1669, -2.3713,\n",
      "         -2.4333, -2.6145],\n",
      "        [-2.1229, -2.2428, -2.0529, -2.6180, -2.4553, -2.7154, -2.2826, -2.3646,\n",
      "         -2.0272, -2.3737],\n",
      "        [-2.2126, -2.0986, -2.1343, -2.5648, -2.4886, -2.6520, -1.9906, -2.4576,\n",
      "         -2.2812, -2.3566],\n",
      "        [-2.1240, -2.0546, -2.3065, -2.2089, -2.3666, -2.4887, -2.3805, -2.6085,\n",
      "         -2.2114, -2.4038],\n",
      "        [-2.3131, -2.1641, -2.2339, -2.4263, -2.3461, -2.3834, -2.2110, -2.4541,\n",
      "         -2.1675, -2.3779],\n",
      "        [-2.1559, -2.2326, -2.2110, -2.3500, -2.4296, -2.5110, -2.3181, -2.1712,\n",
      "         -2.4164, -2.2928],\n",
      "        [-2.0492, -1.9857, -2.0244, -1.7896, -3.1785, -2.4732, -2.7769, -2.4301,\n",
      "         -2.3170, -2.8134],\n",
      "        [-2.0401, -2.2959, -2.1558, -2.4056, -2.6770, -2.4990, -2.1899, -2.2597,\n",
      "         -2.1604, -2.5163],\n",
      "        [-2.0736, -2.1724, -2.2665, -2.2865, -2.3150, -2.4681, -2.4471, -2.4072,\n",
      "         -2.2629, -2.3980],\n",
      "        [-2.1359, -2.3398, -2.2076, -2.1867, -2.2325, -2.3600, -2.4815, -2.3290,\n",
      "         -2.3368, -2.4778],\n",
      "        [-2.2123, -2.2159, -2.1905, -2.3889, -2.3363, -2.3839, -2.2416, -2.3887,\n",
      "         -2.2859, -2.4155],\n",
      "        [-2.1521, -2.2563, -2.2323, -2.2762, -2.6968, -2.5831, -2.3434, -1.9751,\n",
      "         -2.4486, -2.2542],\n",
      "        [-1.8798, -2.1945, -2.0030, -2.1114, -2.7084, -2.7689, -2.3749, -2.2803,\n",
      "         -2.7808, -2.3715],\n",
      "        [-1.9053, -2.4360, -2.0413, -2.2959, -2.7220, -2.2694, -2.2866, -2.5298,\n",
      "         -2.5454, -2.2601],\n",
      "        [-2.2117, -2.1989, -2.2712, -2.1270, -2.3155, -2.1946, -2.4650, -2.2256,\n",
      "         -2.5141, -2.6158],\n",
      "        [-2.2081, -2.1799, -2.2773, -2.3089, -2.3129, -2.4070, -2.3269, -2.3958,\n",
      "         -2.3772, -2.2581],\n",
      "        [-1.8657, -2.1163, -2.2129, -2.1431, -2.4161, -2.6646, -2.4825, -2.5878,\n",
      "         -2.4302, -2.3813],\n",
      "        [-2.2267, -2.2026, -2.0852, -2.3536, -2.4601, -2.4292, -2.4277, -2.3256,\n",
      "         -2.2520, -2.3269],\n",
      "        [-2.2739, -2.3477, -2.2691, -2.0657, -2.1984, -2.5425, -2.3496, -2.3822,\n",
      "         -2.2435, -2.4318],\n",
      "        [-2.1067, -2.2938, -2.3086, -2.3541, -2.0968, -2.5966, -2.3531, -2.3707,\n",
      "         -2.2866, -2.3472],\n",
      "        [-2.1155, -2.3347, -2.1347, -2.3583, -2.3241, -2.5551, -2.4016, -2.5702,\n",
      "         -2.3121, -2.0555],\n",
      "        [-2.0813, -2.1767, -2.0623, -1.7683, -2.8886, -2.6601, -2.6307, -2.3732,\n",
      "         -2.2645, -2.6688],\n",
      "        [-2.1661, -2.3735, -2.1735, -2.2775, -2.2997, -2.5071, -2.2553, -2.3530,\n",
      "         -2.3316, -2.3326],\n",
      "        [-1.9612, -2.2969, -2.3707, -2.1563, -2.2737, -2.4721, -2.6166, -2.2323,\n",
      "         -2.4987, -2.3055],\n",
      "        [-1.9868, -2.3320, -2.1816, -2.0495, -2.4055, -2.5528, -2.3723, -2.3817,\n",
      "         -2.3476, -2.5934],\n",
      "        [-1.9879, -2.0409, -2.4090, -2.2187, -2.2748, -2.5783, -2.5040, -2.4014,\n",
      "         -2.5501, -2.2506],\n",
      "        [-2.3080, -2.0943, -2.3307, -2.1207, -2.2935, -2.6743, -2.1869, -2.4807,\n",
      "         -2.4487, -2.2258],\n",
      "        [-2.2200, -2.1386, -2.1324, -2.3001, -2.4538, -2.4234, -2.4281, -2.3569,\n",
      "         -2.3331, -2.2998],\n",
      "        [-2.1755, -2.0999, -2.2107, -2.2536, -2.3605, -2.2743, -2.5265, -2.5299,\n",
      "         -2.4469, -2.2450],\n",
      "        [-1.7531, -2.4304, -2.1728, -2.5074, -2.7296, -2.3333, -2.3982, -2.6653,\n",
      "         -2.3582, -2.0703],\n",
      "        [-1.9809, -2.2605, -2.1477, -2.3602, -2.4296, -2.3332, -2.4179, -2.3113,\n",
      "         -2.3898, -2.5053],\n",
      "        [-2.2519, -2.2334, -2.2423, -2.3272, -2.3310, -2.4224, -2.2986, -2.4592,\n",
      "         -2.2786, -2.2107],\n",
      "        [-2.1830, -2.1332, -2.3351, -2.1883, -2.3440, -2.3475, -2.4683, -2.4354,\n",
      "         -2.2742, -2.3730],\n",
      "        [-2.2735, -2.1945, -2.2720, -2.3624, -2.3161, -2.3704, -2.2524, -2.4644,\n",
      "         -2.2790, -2.2669],\n",
      "        [-1.9199, -2.0451, -2.1945, -2.0462, -2.3900, -2.5293, -2.4667, -2.6265,\n",
      "         -2.5126, -2.6065],\n",
      "        [-2.1781, -2.3884, -2.1106, -2.1905, -2.3198, -2.4858, -2.5043, -2.6510,\n",
      "         -2.1839, -2.1586],\n",
      "        [-2.0154, -2.2406, -2.4226, -2.0880, -2.3680, -2.4771, -2.4129, -2.5399,\n",
      "         -2.4107, -2.1904],\n",
      "        [-2.2303, -2.2317, -2.2164, -2.3530, -2.3541, -2.3733, -2.3310, -2.3576,\n",
      "         -2.2877, -2.3067],\n",
      "        [-2.0822, -2.1964, -2.2276, -2.1711, -2.5158, -2.3405, -2.4203, -2.4342,\n",
      "         -2.2657, -2.4663],\n",
      "        [-2.1200, -2.2414, -2.0199, -2.2016, -2.6218, -2.2312, -2.4186, -2.6253,\n",
      "         -2.3929, -2.3282],\n",
      "        [-2.1863, -2.2096, -2.2885, -2.2347, -2.3952, -2.3994, -2.3372, -2.3555,\n",
      "         -2.3787, -2.2688],\n",
      "        [-1.9716, -2.2203, -2.3175, -2.4191, -2.4587, -2.4588, -2.3751, -2.3512,\n",
      "         -2.4189, -2.1514],\n",
      "        [-2.1657, -2.0350, -2.0090, -2.1111, -2.8482, -2.3770, -2.4012, -2.5636,\n",
      "         -2.0674, -2.9098],\n",
      "        [-1.9570, -1.9641, -2.1511, -2.3862, -2.6467, -2.8610, -2.3019, -2.4528,\n",
      "         -2.1430, -2.5435],\n",
      "        [-2.0579, -2.2726, -2.0036, -2.5074, -2.4083, -2.4223, -2.3914, -2.2709,\n",
      "         -2.3694, -2.4537],\n",
      "        [-2.1754, -2.2644, -2.2964, -2.2605, -2.2721, -2.4926, -2.2813, -2.3207,\n",
      "         -2.2454, -2.4580],\n",
      "        [-1.9313, -2.1411, -2.1324, -2.6645, -2.6344, -2.5267, -2.3147, -2.4066,\n",
      "         -2.2799, -2.2404],\n",
      "        [-2.1309, -1.9641, -2.4729, -2.4588, -2.4607, -2.6272, -2.2462, -2.2180,\n",
      "         -2.3014, -2.3155],\n",
      "        [-2.0362, -2.2299, -2.1319, -2.3955, -2.2695, -2.4292, -2.4362, -2.2531,\n",
      "         -2.5287, -2.4259],\n",
      "        [-1.7422, -2.4352, -2.1111, -2.2502, -2.4220, -2.5955, -2.2354, -2.4725,\n",
      "         -2.7697, -2.3705],\n",
      "        [-2.0566, -2.3148, -2.3415, -2.3274, -2.2131, -2.2917, -2.3109, -2.5388,\n",
      "         -2.3340, -2.3636],\n",
      "        [-2.1954, -2.3000, -2.2128, -2.2872, -2.4801, -2.5573, -2.2493, -2.2955,\n",
      "         -2.3296, -2.1829],\n",
      "        [-1.9607, -2.2645, -2.0924, -2.1613, -2.6185, -2.2177, -2.4646, -2.5500,\n",
      "         -2.4920, -2.4175],\n",
      "        [-2.2620, -2.1899, -2.2076, -2.0175, -2.6356, -2.3667, -2.2921, -2.5315,\n",
      "         -2.2354, -2.4308],\n",
      "        [-2.0356, -2.1820, -2.2429, -2.3763, -2.3601, -2.4885, -2.3896, -2.4248,\n",
      "         -2.2421, -2.3678]], grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "for idx, (xb, yb) in enumerate(test_loader):\n",
    "    if idx == 0:\n",
    "        preds = model(xb)\n",
    "        argmax = torch.argmax(preds,dim=1)\n",
    "        print(argmax)\n",
    "        print(preds)\n",
    "        print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
