{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Estimator API Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aistudio.estimator3 import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'aistudio.estimator3.Estimator'>\n"
     ]
    }
   ],
   "source": [
    "print(Estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sky/dev/aistudio/workspace/ws-1', '/home/sky/anaconda3/envs/ai/lib/python37.zip', '/home/sky/anaconda3/envs/ai/lib/python3.7', '/home/sky/anaconda3/envs/ai/lib/python3.7/lib-dynload', '', '/home/sky/anaconda3/envs/ai/lib/python3.7/site-packages', '/home/sky/anaconda3/envs/ai/lib/python3.7/site-packages/IPython/extensions', '/home/sky/.ipython', '../../']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../aistudio/estimator3.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getfile(Estimator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "download_root = 'train-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                           ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ])\n",
    "train_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "input_data_torch = train_dataset.data\n",
    "input_data_torch = torch.div(input_data_torch,255.)\n",
    "input_data_torch = torch.add(input_data_torch,-0.1307)\n",
    "input_data_torch = torch.div(input_data_torch,0.3081)\n",
    "input_data_torch = input_data_torch.unsqueeze(1)\n",
    "input_labels_torch = train_dataset.targets\n",
    "# Convert Tensor to Numpy\n",
    "input_data = input_data_torch.numpy()\n",
    "input_labels = input_labels_torch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Check data type and shape\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(input_data))\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-info\">\n",
    "    Set Script Parameters\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':64,\n",
    "    'test-batch-size':128,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    #'no-cuda': True, # True is dummy value, value is always True\n",
    "    'nprocs':1,\n",
    "    #'loss':'cross_entropy',\n",
    "    'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True # True is dummy value, value is always True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1 (with model name in the aistudio service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(model_name=\"model-1\",script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(input_data,input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile './estimator3_test.py'\n",
    "\n",
    "# from aistudio.estimator3 import Estimator\n",
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# download_root = 'train-data'\n",
    "# mnist_transform=transforms.Compose([\n",
    "#                            transforms.ToTensor()\n",
    "#                            ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                            ])\n",
    "# train_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "# input_data_torch = train_dataset.data\n",
    "# input_data_torch = torch.div(input_data_torch,255.)\n",
    "# input_data_torch = torch.add(input_data_torch,-0.1307)\n",
    "# input_data_torch = torch.div(input_data_torch,0.3081)\n",
    "# input_data_torch = input_data_torch.unsqueeze(1)\n",
    "# input_labels_torch = train_dataset.targets\n",
    "# # Convert Tensor to Numpy\n",
    "# input_data = input_data_torch.numpy()\n",
    "# input_labels = input_labels_torch.numpy()\n",
    "# script_params = {\n",
    "#     'epochs':5,\n",
    "#     'batch-size':64,\n",
    "#     'test-batch-size':128,\n",
    "#     'lr':0.01,\n",
    "#     'momentum':0.5,\n",
    "#     'seed':42,\n",
    "#     'log-interval':10,\n",
    "#     'no-cuda':False,\n",
    "#     'nprocs':1,\n",
    "#     'loss':'cross_entropy',\n",
    "#     'optimizer':'SGD'\n",
    "# }\n",
    "# estimator = Estimator(model_name=\"model-1\",script_params=script_params)\n",
    "# estimator.fit(input_data,input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2 (with model object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make network and load model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"./mnist_net.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(model_name=\"model-2\",script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(input_data,input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"torchnet\"\n",
    "######### DO NOT CHANGE #########\n",
    "filename = modulename + \".py\"\n",
    "filename = \"./nets/\" + filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':64,\n",
    "    'test-batch-size':128,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    'no-cuda':False,\n",
    "    'nprocs':1,\n",
    "    #'loss':'cross_entropy',\n",
    "    'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(input_data,input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Scenario2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Example 1(MNIST-deeplearning.net)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"mnist-linear\"\n",
    "######### DO NOT CHANGE #########\n",
    "net_filename = modulename + \".py\"\n",
    "net_filename = \"./nets/\" + net_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $net_filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "dataset_name = \"MNIST\"\n",
    "######### DO NOT CHANGE #########\n",
    "dataset_filename = dataset_name + \".py\"\n",
    "dataset_filename = \"./datasets/\" + dataset_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $dataset_filename\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class DatasetLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        ########## WRITE DATASET LOADER CODE HERE ##########\n",
    "        \n",
    "        DATA_PATH = Path(\"datasets\")\n",
    "        PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "        PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        URL = \"http://deeplearning.net/data/mnist/\"\n",
    "        FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "        if not (PATH / FILENAME).exists():\n",
    "                content = requests.get(URL + FILENAME).content\n",
    "                (PATH / FILENAME).open(\"wb\").write(content)\n",
    "\n",
    "        with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "                ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")       \n",
    "\n",
    "        x_train, y_train, x_valid, y_valid = map(\n",
    "            torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "        )\n",
    "        ### train_ds and valid_ds MUST BE TensorDataset(or ImageFolder or Torch Dataset Format)\n",
    "        self.train_dataset = TensorDataset(x_train, y_train)\n",
    "        self.valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "        ####################################################\n",
    "        \n",
    "    def get_train_dataset(self, validation=True):        \n",
    "        if validation is True:\n",
    "            return self.train_dataset, self.valid_dataset\n",
    "        else:\n",
    "            return self.train_dataset\n",
    "    \n",
    "    def get_test_dataset(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':64,\n",
    "    'test-batch-size':128,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    'no-cuda':False,\n",
    "    'nprocs':1,\n",
    "    'loss':'cross_entropy',\n",
    "    #'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(dataset_loader=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Example 2(MNIST-torchvision)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"mnist-CNN\"\n",
    "######### DO NOT CHANGE #########\n",
    "net_filename = modulename + \".py\"\n",
    "net_filename = \"./nets/\" + net_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $net_filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "dataset_name = \"MNIST2\"\n",
    "######### DO NOT CHANGE #########\n",
    "dataset_filename = dataset_name + \".py\"\n",
    "dataset_filename = \"./datasets/\" + dataset_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $dataset_filename\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class DatasetLoader:    \n",
    "    def __init__(self):\n",
    "        ########## WRITE DATASET LOADER CODE HERE ##########\n",
    "        thispath = os.path.dirname(os.path.abspath(__file__))\n",
    "        data_dir = os.path.join(thispath,\"MNIST2\")\n",
    "        mnist_transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train_dataset = datasets.MNIST(data_dir, transform=mnist_transform, train=True,  download=True)\n",
    "        test_dataset  = datasets.MNIST(data_dir, transform=mnist_transform, train=False, download=True)\n",
    "        \n",
    "        ### train_ds and valid_ds MUST BE TensorDataset(or ImageFolder or Torch Dataset Format)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        ####################################################\n",
    "    \n",
    "    def get_train_dataset(self, validation=True):        \n",
    "        if validation is True:\n",
    "            return self.train_dataset, self.valid_dataset\n",
    "        else:\n",
    "            return self.train_dataset\n",
    "    \n",
    "    def get_test_dataset(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':64,\n",
    "    'test-batch-size':128,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    #'no-cuda':False,\n",
    "    'nprocs':1,\n",
    "    #'loss':'cross_entropy',\n",
    "    'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(dataset_loader=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Example 3(CIFAR10)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"CIFAR10-CNN\"\n",
    "######### DO NOT CHANGE #########\n",
    "net_filename = modulename + \".py\"\n",
    "net_filename = \"./nets/\" + net_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $net_filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "dataset_name = \"CIFAR10\"\n",
    "######### DO NOT CHANGE #########\n",
    "dataset_filename = dataset_name + \".py\"\n",
    "dataset_filename = \"./datasets/\" + dataset_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $dataset_filename\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "\n",
    "class DatasetLoader:    \n",
    "    def __init__(self):\n",
    "        ########## WRITE DATASET LOADER CODE HERE ##########\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        thispath = os.path.dirname(os.path.abspath(__file__))\n",
    "        data_dir = os.path.join(thispath,\"CIFAR10\")\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                                download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                               download=True, transform=transform)\n",
    "        self.train_dataset = trainset\n",
    "        self.test_dataset = testset\n",
    "        ####################################################\n",
    "        \n",
    "    def get_train_dataset(self, validation=True):        \n",
    "        if validation is True:\n",
    "            return self.train_dataset, self.valid_dataset\n",
    "        else:\n",
    "            return self.train_dataset\n",
    "    \n",
    "    def get_test_dataset(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':32,\n",
    "    'test-batch-size':64,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.9,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    #'no-cuda':False,\n",
    "    'nprocs':1,\n",
    "    'loss':'cross_entropy',\n",
    "    #'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(dataset_loader=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Example 4(hymenoptera)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"hymenoptera\"\n",
    "######### DO NOT CHANGE #########\n",
    "net_filename = modulename + \".py\"\n",
    "net_filename = \"./nets/\" + net_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./nets/hymenoptera.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $net_filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "dataset_name = \"hymenoptera\"\n",
    "######### DO NOT CHANGE #########\n",
    "dataset_filename = dataset_name + \".py\"\n",
    "dataset_filename = \"./datasets/\" + dataset_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./datasets/hymenoptera.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dataset_filename\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "\n",
    "class DatasetLoader:    \n",
    "    def __init__(self):\n",
    "        ########## WRITE DATASET LOADER CODE HERE ##########\n",
    "        # Just normalization for validation\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "        thispath = os.path.dirname(os.path.abspath(__file__))\n",
    "        data_dir = os.path.join(thispath,\"hymenoptera_data\")        \n",
    "        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                                  data_transforms[x])\n",
    "                          for x in ['train', 'val']}\n",
    "        self.train_dataset = image_datasets['train']\n",
    "        self.valid_dataset = image_datasets['val']\n",
    "        ####################################################\n",
    "        \n",
    "    def get_train_dataset(self, validation=True):        \n",
    "        if validation is True:\n",
    "            return self.train_dataset, self.valid_dataset\n",
    "        else:\n",
    "            return self.train_dataset\n",
    "    \n",
    "    def get_test_dataset(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':5,\n",
    "    'batch-size':64,\n",
    "    'test-batch-size':128,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    #'no-cuda':False,\n",
    "    'nprocs':1,\n",
    "    #'loss':'cross_entropy',\n",
    "    'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--epochs', '5', '--batch-size', '64', '--test-batch-size', '128', '--lr', '0.01', '--momentum', '0.5', '--seed', '42', '--log-interval', '10', '--nprocs', '1', '--loss', 'nll_loss', '--optimizer', 'SGD', '--debug', '--net-name', 'hymenoptera']\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Requested to Portal.\n",
      "[1,0]<stdout>:Namespace(batch_size=64, dataset_loader='hymenoptera', debug=True, epochs=5, log_interval=10, loss='nll_loss', lr=0.01, model_path=None, momentum=0.5, net_name='hymenoptera', no_cuda=False, nprocs=1, optimizer='SGD', seed=42, test_batch_size=128, use_adasum=False)\n",
      "[1,0]<stdout>:Arguments Parsing Finished.\n",
      "[1,0]<stdout>:CUDA Supported!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(dataset_loader=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Example 5(MNIST-.csv, .png)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "modulename = \"mnist-csv\"\n",
    "######### DO NOT CHANGE #########\n",
    "net_filename = modulename + \".py\"\n",
    "net_filename = \"./nets/\" + net_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Do not change code below at Line 1 to 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./nets/mnist-csv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $net_filename\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-DEFINED - Feel free to change\n",
    "dataset_name = \"mnist-images\"\n",
    "######### DO NOT CHANGE #########\n",
    "dataset_filename = dataset_name + \".py\"\n",
    "dataset_filename = \"./datasets/\" + dataset_filename\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./datasets/mnist-images.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dataset_filename\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "class DatasetLoader:    \n",
    "    def __init__(self):\n",
    "        ########## WRITE DATASET LOADER CODE HERE ##########\n",
    "        # Just normalization for validation        \n",
    "        mnist_transform=transforms.Compose([transforms.Grayscale(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        thispath = os.path.dirname(os.path.abspath(__file__))\n",
    "        data_dir = os.path.join(thispath,\"image-datasets\",\"mnist\",\"images\")\n",
    "        image_dataset = torchvision.datasets.ImageFolder(data_dir,mnist_transform)\n",
    "        \n",
    "        self.train_dataset = image_dataset\n",
    "        ####################################################\n",
    "        \n",
    "    def get_train_dataset(self, validation=True):        \n",
    "        if validation is True:\n",
    "            return self.train_dataset, self.valid_dataset\n",
    "        else:\n",
    "            return self.train_dataset\n",
    "    \n",
    "    def get_test_dataset(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    'epochs':1,\n",
    "    'batch-size':32,\n",
    "    'test-batch-size':64,\n",
    "    'lr':0.01,\n",
    "    'momentum':0.5,\n",
    "    'seed':42,\n",
    "    'log-interval':10,\n",
    "    'no-cuda':True,\n",
    "    'nprocs':1,\n",
    "    #'loss':'cross_entropy',\n",
    "    'loss':'nll_loss',\n",
    "    'optimizer':'SGD',\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--epochs', '1', '--batch-size', '32', '--test-batch-size', '64', '--lr', '0.01', '--momentum', '0.5', '--seed', '42', '--log-interval', '10', '--no-cuda', '--nprocs', '1', '--loss', 'nll_loss', '--optimizer', 'SGD', '--debug', '--net-name', 'mnist-csv']\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(net_name=modulename,script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Requested to Portal.\n",
      "[1,0]<stdout>:Namespace(batch_size=32, dataset_loader='mnist-images', debug=True, epochs=1, log_interval=10, loss='nll_loss', lr=0.01, model_path=None, momentum=0.5, net_name='mnist-csv', no_cuda=True, nprocs=1, optimizer='SGD', seed=42, test_batch_size=64, use_adasum=False)\n",
      "[1,0]<stdout>:Arguments Parsing Finished.\n",
      "[1,0]<stdout>:CUDA Not Supported!\n",
      "[1,0]<stdout>:Network was found.\n",
      "[1,0]<stdout>:Model's state_dict:\n",
      "[1,0]<stdout>:conv1.weight \t torch.Size([10, 1, 5, 5])\n",
      "[1,0]<stdout>:conv1.bias \t torch.Size([10])\n",
      "[1,0]<stdout>:conv2.weight \t torch.Size([20, 10, 5, 5])\n",
      "[1,0]<stdout>:conv2.bias \t torch.Size([20])\n",
      "[1,0]<stdout>:fc1.weight \t torch.Size([50, 320])\n",
      "[1,0]<stdout>:fc1.bias \t torch.Size([50])\n",
      "[1,0]<stdout>:fc2.weight \t torch.Size([10, 50])\n",
      "[1,0]<stdout>:fc2.bias \t torch.Size([10])\n",
      "[1,0]<stdout>:Optimizer's state_dict:\n",
      "[1,0]<stdout>:state \t {}\n",
      "[1,0]<stdout>:param_groups \t [{'lr': 0.01, 'momentum': 0.5, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [140423637613056, 140423521441824, 140423521442304, 140423521442384, 140423521442464, 140423521442544, 140423521442704, 140423521442784]}]\n",
      "[1,0]<stdout>:Train Epoch: 1 [0/70001 (0%)]\tLoss: 2.359439\tAccuracy: 0.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [320/70001 (0%)]\tLoss: 2.303537\tAccuracy: 9.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [640/70001 (1%)]\tLoss: 2.278645\tAccuracy: 15.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [960/70001 (1%)]\tLoss: 2.301762\tAccuracy: 0.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [1280/70001 (2%)]\tLoss: 2.291786\tAccuracy: 3.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [1600/70001 (2%)]\tLoss: 2.261696\tAccuracy: 12.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [1920/70001 (3%)]\tLoss: 2.273242\tAccuracy: 15.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [2240/70001 (3%)]\tLoss: 2.291846\tAccuracy: 6.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [2560/70001 (4%)]\tLoss: 2.288336\tAccuracy: 21.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [2880/70001 (4%)]\tLoss: 2.252943\tAccuracy: 15.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [3200/70001 (5%)]\tLoss: 2.195977\tAccuracy: 25.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [3520/70001 (5%)]\tLoss: 2.197427\tAccuracy: 21.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [3840/70001 (5%)]\tLoss: 2.160396\tAccuracy: 18.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [4160/70001 (6%)]\tLoss: 2.107425\tAccuracy: 28.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [4480/70001 (6%)]\tLoss: 2.223942\tAccuracy: 12.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [4800/70001 (7%)]\tLoss: 2.022172\tAccuracy: 31.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [5120/70001 (7%)]\tLoss: 2.021240\tAccuracy: 34.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [5440/70001 (8%)]\tLoss: 1.647147\tAccuracy: 40.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [5760/70001 (8%)]\tLoss: 1.390833\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [6080/70001 (9%)]\tLoss: 1.938852\tAccuracy: 31.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [6400/70001 (9%)]\tLoss: 1.727454\tAccuracy: 46.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [6720/70001 (10%)]\tLoss: 1.243750\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [7040/70001 (10%)]\tLoss: 1.290093\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [7360/70001 (11%)]\tLoss: 0.940538\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [7680/70001 (11%)]\tLoss: 1.177681\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [8000/70001 (11%)]\tLoss: 1.018751\tAccuracy: 65.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [8320/70001 (12%)]\tLoss: 1.278147\tAccuracy: 53.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [8640/70001 (12%)]\tLoss: 1.169076\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [8960/70001 (13%)]\tLoss: 0.870117\tAccuracy: 65.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [9280/70001 (13%)]\tLoss: 1.045447\tAccuracy: 62.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [9600/70001 (14%)]\tLoss: 0.807363\tAccuracy: 71.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [9920/70001 (14%)]\tLoss: 0.826652\tAccuracy: 62.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [10240/70001 (15%)]\tLoss: 0.948322\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [10560/70001 (15%)]\tLoss: 0.941050\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [10880/70001 (16%)]\tLoss: 0.753810\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [11200/70001 (16%)]\tLoss: 0.599389\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [11520/70001 (16%)]\tLoss: 0.610819\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [11840/70001 (17%)]\tLoss: 0.853904\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [12160/70001 (17%)]\tLoss: 0.807191\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [12480/70001 (18%)]\tLoss: 0.959571\tAccuracy: 62.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [12800/70001 (18%)]\tLoss: 0.863811\tAccuracy: 71.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [13120/70001 (19%)]\tLoss: 0.777452\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [13440/70001 (19%)]\tLoss: 0.775526\tAccuracy: 65.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [13760/70001 (20%)]\tLoss: 0.703041\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [14080/70001 (20%)]\tLoss: 0.760835\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [14400/70001 (21%)]\tLoss: 0.640970\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [14720/70001 (21%)]\tLoss: 0.900737\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [15040/70001 (21%)]\tLoss: 0.837805\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [15360/70001 (22%)]\tLoss: 0.695556\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [15680/70001 (22%)]\tLoss: 0.580451\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [16000/70001 (23%)]\tLoss: 0.660031\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [16320/70001 (23%)]\tLoss: 0.650316\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [16640/70001 (24%)]\tLoss: 1.056228\tAccuracy: 62.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [16960/70001 (24%)]\tLoss: 0.538144\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [17280/70001 (25%)]\tLoss: 0.804427\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [17600/70001 (25%)]\tLoss: 0.438857\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [17920/70001 (26%)]\tLoss: 0.929751\tAccuracy: 62.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [18240/70001 (26%)]\tLoss: 0.533188\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [18560/70001 (27%)]\tLoss: 0.591935\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [18880/70001 (27%)]\tLoss: 0.900773\tAccuracy: 59.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [19200/70001 (27%)]\tLoss: 0.543820\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [19520/70001 (28%)]\tLoss: 0.755215\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [19840/70001 (28%)]\tLoss: 1.147322\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [20160/70001 (29%)]\tLoss: 0.559129\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [20480/70001 (29%)]\tLoss: 0.714214\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [20800/70001 (30%)]\tLoss: 0.550702\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [21120/70001 (30%)]\tLoss: 0.532928\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [21440/70001 (31%)]\tLoss: 0.471865\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [21760/70001 (31%)]\tLoss: 0.506051\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [22080/70001 (32%)]\tLoss: 0.648377\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [22400/70001 (32%)]\tLoss: 0.605991\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [22720/70001 (32%)]\tLoss: 0.799348\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [23040/70001 (33%)]\tLoss: 0.879702\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [23360/70001 (33%)]\tLoss: 0.548660\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [23680/70001 (34%)]\tLoss: 0.656103\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [24000/70001 (34%)]\tLoss: 0.454837\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [24320/70001 (35%)]\tLoss: 0.256317\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [24640/70001 (35%)]\tLoss: 0.608683\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [24960/70001 (36%)]\tLoss: 0.447858\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [25280/70001 (36%)]\tLoss: 0.228151\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [25600/70001 (37%)]\tLoss: 0.542932\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [25920/70001 (37%)]\tLoss: 0.740446\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [26240/70001 (37%)]\tLoss: 0.387598\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [26560/70001 (38%)]\tLoss: 0.678201\tAccuracy: 68.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [26880/70001 (38%)]\tLoss: 0.796502\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [27200/70001 (39%)]\tLoss: 0.451205\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [27520/70001 (39%)]\tLoss: 0.544192\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [27840/70001 (40%)]\tLoss: 0.670159\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [28160/70001 (40%)]\tLoss: 0.751226\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [28480/70001 (41%)]\tLoss: 0.696778\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [28800/70001 (41%)]\tLoss: 0.501688\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [29120/70001 (42%)]\tLoss: 0.627674\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [29440/70001 (42%)]\tLoss: 0.224773\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [29760/70001 (43%)]\tLoss: 0.703339\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [30080/70001 (43%)]\tLoss: 0.309421\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [30400/70001 (43%)]\tLoss: 0.388740\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [30720/70001 (44%)]\tLoss: 0.231002\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [31040/70001 (44%)]\tLoss: 0.539798\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [31360/70001 (45%)]\tLoss: 0.640454\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [31680/70001 (45%)]\tLoss: 0.467581\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [32000/70001 (46%)]\tLoss: 0.512756\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [32320/70001 (46%)]\tLoss: 0.416619\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [32640/70001 (47%)]\tLoss: 0.622256\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [32960/70001 (47%)]\tLoss: 0.801393\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [33280/70001 (48%)]\tLoss: 0.385790\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [33600/70001 (48%)]\tLoss: 0.510257\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [33920/70001 (48%)]\tLoss: 0.472899\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [34240/70001 (49%)]\tLoss: 0.378743\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [34560/70001 (49%)]\tLoss: 0.734666\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [34880/70001 (50%)]\tLoss: 0.427296\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [35200/70001 (50%)]\tLoss: 0.509856\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [35520/70001 (51%)]\tLoss: 0.574644\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [35840/70001 (51%)]\tLoss: 0.445646\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [36160/70001 (52%)]\tLoss: 0.348464\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [36480/70001 (52%)]\tLoss: 0.598104\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [36800/70001 (53%)]\tLoss: 0.710479\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [37120/70001 (53%)]\tLoss: 0.500671\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [37440/70001 (53%)]\tLoss: 0.378111\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [37760/70001 (54%)]\tLoss: 0.394579\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [38080/70001 (54%)]\tLoss: 0.446567\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [38400/70001 (55%)]\tLoss: 0.212397\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [38720/70001 (55%)]\tLoss: 0.334554\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [39040/70001 (56%)]\tLoss: 0.358590\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [39360/70001 (56%)]\tLoss: 0.364152\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [39680/70001 (57%)]\tLoss: 0.415377\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [40000/70001 (57%)]\tLoss: 1.100297\tAccuracy: 71.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [40320/70001 (58%)]\tLoss: 0.405301\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [40640/70001 (58%)]\tLoss: 0.416509\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [40960/70001 (59%)]\tLoss: 0.257843\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [41280/70001 (59%)]\tLoss: 0.505907\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [41600/70001 (59%)]\tLoss: 0.207907\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [41920/70001 (60%)]\tLoss: 0.486380\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [42240/70001 (60%)]\tLoss: 0.350853\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [42560/70001 (61%)]\tLoss: 0.576401\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [42880/70001 (61%)]\tLoss: 0.389900\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [43200/70001 (62%)]\tLoss: 0.335337\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [43520/70001 (62%)]\tLoss: 0.292426\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [43840/70001 (63%)]\tLoss: 0.201769\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [44160/70001 (63%)]\tLoss: 0.623155\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [44480/70001 (64%)]\tLoss: 0.469498\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [44800/70001 (64%)]\tLoss: 0.404636\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [45120/70001 (64%)]\tLoss: 0.376447\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [45440/70001 (65%)]\tLoss: 0.428249\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [45760/70001 (65%)]\tLoss: 0.528909\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [46080/70001 (66%)]\tLoss: 0.534158\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [46400/70001 (66%)]\tLoss: 0.356943\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [46720/70001 (67%)]\tLoss: 0.458808\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [47040/70001 (67%)]\tLoss: 0.176711\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [47360/70001 (68%)]\tLoss: 0.351524\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [47680/70001 (68%)]\tLoss: 0.518570\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [48000/70001 (69%)]\tLoss: 0.531030\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [48320/70001 (69%)]\tLoss: 0.520061\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [48640/70001 (69%)]\tLoss: 0.312403\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [48960/70001 (70%)]\tLoss: 0.135165\tAccuracy: 100.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [49280/70001 (70%)]\tLoss: 0.349812\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [49600/70001 (71%)]\tLoss: 0.501267\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [49920/70001 (71%)]\tLoss: 0.920748\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [50240/70001 (72%)]\tLoss: 0.734616\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [50560/70001 (72%)]\tLoss: 0.442554\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [50880/70001 (73%)]\tLoss: 0.627410\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [51200/70001 (73%)]\tLoss: 0.184920\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [51520/70001 (74%)]\tLoss: 0.328689\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [51840/70001 (74%)]\tLoss: 0.793502\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [52160/70001 (74%)]\tLoss: 0.618252\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [52480/70001 (75%)]\tLoss: 0.559092\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [52800/70001 (75%)]\tLoss: 0.345743\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [53120/70001 (76%)]\tLoss: 0.253789\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [53440/70001 (76%)]\tLoss: 0.453720\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [53760/70001 (77%)]\tLoss: 0.823557\tAccuracy: 75.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [54080/70001 (77%)]\tLoss: 0.182490\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [54400/70001 (78%)]\tLoss: 0.418042\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [54720/70001 (78%)]\tLoss: 0.475722\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [55040/70001 (79%)]\tLoss: 0.220839\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [55360/70001 (79%)]\tLoss: 0.159634\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [55680/70001 (80%)]\tLoss: 0.558052\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [56000/70001 (80%)]\tLoss: 0.220301\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [56320/70001 (80%)]\tLoss: 0.154360\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [56640/70001 (81%)]\tLoss: 0.177626\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [56960/70001 (81%)]\tLoss: 0.967898\tAccuracy: 71.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [57280/70001 (82%)]\tLoss: 0.235371\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [57600/70001 (82%)]\tLoss: 0.262935\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [57920/70001 (83%)]\tLoss: 0.331336\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [58240/70001 (83%)]\tLoss: 0.388422\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [58560/70001 (84%)]\tLoss: 0.274213\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [58880/70001 (84%)]\tLoss: 0.853963\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [59200/70001 (85%)]\tLoss: 0.120090\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [59520/70001 (85%)]\tLoss: 0.543715\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [59840/70001 (85%)]\tLoss: 0.247233\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [60160/70001 (86%)]\tLoss: 0.162573\tAccuracy: 100.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [60480/70001 (86%)]\tLoss: 0.185238\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [60800/70001 (87%)]\tLoss: 0.317914\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [61120/70001 (87%)]\tLoss: 0.271733\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [61440/70001 (88%)]\tLoss: 0.306431\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [61760/70001 (88%)]\tLoss: 0.349150\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [62080/70001 (89%)]\tLoss: 0.547119\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [62400/70001 (89%)]\tLoss: 0.455606\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [62720/70001 (90%)]\tLoss: 0.250752\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [63040/70001 (90%)]\tLoss: 0.552226\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [63360/70001 (90%)]\tLoss: 0.267592\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [63680/70001 (91%)]\tLoss: 0.147377\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [64000/70001 (91%)]\tLoss: 0.280503\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [64320/70001 (92%)]\tLoss: 0.595371\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [64640/70001 (92%)]\tLoss: 0.198309\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [64960/70001 (93%)]\tLoss: 0.450782\tAccuracy: 81.25\n",
      "[1,0]<stdout>:Train Epoch: 1 [65280/70001 (93%)]\tLoss: 0.442203\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [65600/70001 (94%)]\tLoss: 0.379864\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [65920/70001 (94%)]\tLoss: 0.423313\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [66240/70001 (95%)]\tLoss: 0.101331\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [66560/70001 (95%)]\tLoss: 0.446767\tAccuracy: 93.75\n",
      "[1,0]<stdout>:Train Epoch: 1 [66880/70001 (96%)]\tLoss: 0.372981\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [67200/70001 (96%)]\tLoss: 0.299112\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [67520/70001 (96%)]\tLoss: 0.711764\tAccuracy: 78.125\n",
      "[1,0]<stdout>:Train Epoch: 1 [67840/70001 (97%)]\tLoss: 0.108363\tAccuracy: 100.0\n",
      "[1,0]<stdout>:Train Epoch: 1 [68160/70001 (97%)]\tLoss: 0.414880\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [68480/70001 (98%)]\tLoss: 0.508681\tAccuracy: 87.5\n",
      "[1,0]<stdout>:Train Epoch: 1 [68800/70001 (98%)]\tLoss: 0.315498\tAccuracy: 90.625\n",
      "[1,0]<stdout>:Train Epoch: 1 [69120/70001 (99%)]\tLoss: 0.436639\tAccuracy: 84.375\n",
      "[1,0]<stdout>:Train Epoch: 1 [69440/70001 (99%)]\tLoss: 0.101738\tAccuracy: 96.875\n",
      "[1,0]<stdout>:Train Epoch: 1 [69760/70001 (100%)]\tLoss: 0.181318\tAccuracy: 93.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(dataset_loader=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
