{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Estimator to run a script to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aistudio.pytorch import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--batch-size': 64,\n",
    "    '--test-batch-size': 128,\n",
    "    '--lr': 0.5,\n",
    "    '--epochs': 10\n",
    "    #,'--nprocs': 4\n",
    "}\n",
    "estimator = PyTorch(script=\"train.py\",script_params=script_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator with Model File(.onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimator needs test-batch-size, test dataloader and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aistudio.estimator import Estimator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "script_params = {\n",
    "    '--test-batch-size': 128\n",
    "}\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "download_root = 'test-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                           ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ])\n",
    "test_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "# Data Preperation\n",
    "input_data = test_dataset.data\n",
    "input_data = torch.add(input_data,-0.1307)\n",
    "input_data = torch.div(input_data,0.3081)\n",
    "input_data = input_data.unsqueeze(1)\n",
    "input_labels = test_dataset.targets\n",
    "\n",
    "estimator = Estimator(use_model=True,model_path=\"mnist_net.onnx\",script_params=script_params)\n",
    "estimator.validate(input_data, input_labels, loss=\"nll_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator with Model File(.pth)\n",
    "\n",
    "Estimator with PyTorch Model File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aistudio.estimator import Estimator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# define script parameters\n",
    "script_params = {\n",
    "    '--test-batch-size': 128\n",
    "}\n",
    "# Data Preparation and Pre-Processing\n",
    "download_root = 'test-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                           ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ])\n",
    "test_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing for testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_data = test_dataset.data\n",
    "input_data = torch.add(input_data,-0.1307)\n",
    "input_data = torch.div(input_data,0.3081)\n",
    "input_data = input_data.unsqueeze(1)\n",
    "input_labels = test_dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate(Test) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "estimator = Estimator(use_model=True,model_path=\"mnist_net.pth\",network=net,script_params=script_params)\n",
    "estimator.validate(input_data, input_labels, loss=\"nll_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_root = 'train-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                           ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ])\n",
    "train_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "input_data = train_dataset.data\n",
    "input_data = torch.add(input_data,-0.1307)\n",
    "input_data = torch.div(input_data,0.3081)\n",
    "input_data = input_data.unsqueeze(1)\n",
    "input_labels = train_dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to use only model file(it couldn't update weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--epochs': 5,\n",
    "    '--batch-size': 64,\n",
    "    '--test-batch-size': 128,\n",
    "    '--lr': 0.05\n",
    "}\n",
    "\n",
    "net = Net()\n",
    "estimator = Estimator(use_model=True,model_path=\"mnist_init_net.pt\",network=net,script_params=script_params,debug=True)\n",
    "estimator.fit(input_data, input_labels, loss=\"nll_loss\", opt=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to use only model file with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "script_params = {\n",
    "    '--epochs': 5,\n",
    "    '--batch-size': 64,\n",
    "    '--test-batch-size': 128,\n",
    "    '--lr': 0.05\n",
    "}\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "estimator = Estimator(use_model=True,model_path=\"mnist_init_net_checkpoint.pt\",network=net,use_optimizer=True,optimizer=optimizer,script_params=script_params,debug=True)\n",
    "estimator.fit(input_data, input_labels, loss=\"nll_loss\", opt=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to use only model file with checkpoint and cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "script_params = {\n",
    "    '--epochs': 5,\n",
    "    '--batch-size': 64,\n",
    "    '--test-batch-size': 128,\n",
    "    '--lr': 0.05\n",
    "}\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "estimator = Estimator(use_cuda=True,use_model=True,model_path=\"mnist_init_net_checkpoint.pt\",network=net,use_optimizer=True,optimizer=optimizer,script_params=script_params,debug=True)\n",
    "estimator.fit(input_data, input_labels, loss=\"nll_loss\", opt=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Make Horovod Running Test Python File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile './test_estimator_hvd.py'\n",
    "\n",
    "from aistudio.estimator import Estimator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "# import horovod.torch as hvd\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "# define script parameters\n",
    "script_params = {\n",
    "    '--test-batch-size': 128\n",
    "}\n",
    "# Data Preparation and Pre-Processing\n",
    "download_root = 'train-data'\n",
    "mnist_transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                           ,transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ])\n",
    "train_dataset  = datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "input_data = train_dataset.data\n",
    "input_data = torch.add(input_data,-0.1307)\n",
    "input_data = torch.div(input_data,0.3081)\n",
    "input_data = input_data.unsqueeze(1)\n",
    "input_labels = train_dataset.targets\n",
    "\n",
    "script_params = {\n",
    "    '--epochs': 5,\n",
    "    '--batch-size': 64,\n",
    "    '--test-batch-size': 128,\n",
    "    '--lr': 0.05\n",
    "}\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "estimator = Estimator(use_cuda=True,use_model=True,model_path=\"mnist_init_net_checkpoint.pt\",network=net,use_optimizer=True,optimizer=optimizer,script_params=script_params)\n",
    "estimator.fit(input_data, input_labels, loss=\"nll_loss\", opt=\"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
